{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####构建ITR树其中包含tokenization部分####\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import pymongo\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, data, tag):\n",
    "        self.data = data\n",
    "        self.tag= tag\n",
    "        self.children = []\n",
    "        self.embedding = []\n",
    "        self.deep = 0\n",
    "\n",
    "# 给树添加边的信息\n",
    "def add_edge(parent, child, edge_data):\n",
    "    parent.children.append((child, edge_data))\n",
    "    \n",
    "\n",
    "def build_ITR_tree(Seqsstate, Seqslog, Seqscall_1):\n",
    "    # 创建节点字典，将调用ID映射到相应的树节点\n",
    "    call_id_to_node = {}\n",
    "    \n",
    "    # 初始化根节点\n",
    "    root = TreeNode(Seqscall_1[0][0],'call')\n",
    "    call_id_to_node[Seqscall_1[0][0]] = root\n",
    "    root.data = Seqscall_1[1][0]\n",
    "    root.deep = 0\n",
    "    # 遍历调用序列以构建主call树\n",
    "    for i in range(1, len(Seqscall_1[0])):\n",
    "        call = Seqscall_1[0][i]\n",
    "        parent_call = Seqscall_1[0][i - 1]\n",
    "        parent_node = call_id_to_node[parent_call]\n",
    "        call_node = TreeNode(call,'call')\n",
    "        call_node.data = Seqscall_1[1][i]\n",
    "        call_node.deep = parent_node.deep +1\n",
    "        # 添加调用之间的边信息\n",
    "        add_edge(parent_node, call_node, f\"Call {parent_call} -> {call}\")\n",
    "        \n",
    "        call_id_to_node[call] = call_node\n",
    "    \n",
    "    # 遍历状态跟踪以构建树 # 这里有问题\n",
    "    i = 0\n",
    "    for state_call in Seqsstate[0]:\n",
    "        state_node = TreeNode(state_call,'state')\n",
    "        parent_call = Seqsstate[1][state_call]\n",
    "        parent_node = call_id_to_node[parent_call]\n",
    "        \n",
    "        # 添加状态与调用之间的边信息\n",
    "        add_edge(parent_node, state_node, f\"State {parent_call} -> {state_call}\")\n",
    "        state_node.data = Seqsstate[2][i]\n",
    "        state_node.deep = parent_node.deep + 1\n",
    "        i = i+1\n",
    "\n",
    "    # 遍历日志跟踪以构建树\n",
    "    i = 0\n",
    "    for log_call in Seqslog[0]:\n",
    "        log_node = TreeNode(log_call,'log')\n",
    "        parent_call = Seqslog[1][log_call]\n",
    "        parent_node = call_id_to_node[parent_call]\n",
    "        \n",
    "        # 添加日志与调用之间的边信息\n",
    "        add_edge(parent_node, log_node, f\"Log {parent_call} -> {log_call}\")\n",
    "        log_node.data = Seqslog[2][i]\n",
    "        log_node.deep = parent_node.deep + 1\n",
    "        i = i+1\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "####Tokenization部分####\n",
    "def tokenize_text(text,node_tag):\n",
    "    if isinstance(text, str):\n",
    "        # 使用逗号和下划线作为分隔符\n",
    "        tokens = re.split(r'[,]', text)\n",
    "        # 仅保留长度大于1的单词\n",
    "        tokens = [token.strip() for token in tokens if len(token) > 1]\n",
    "        # 添加[START]和[END]标记\n",
    "        tokens = ['[START]'] + [f'[{node_tag.upper()}]'] + tokens + ['[END]']\n",
    "        \n",
    "        # 对call_trace中in和out部分添加[OUTs]和[INs]标签\n",
    "        if  node_tag == 'call':\n",
    "            in_indices = [i for i, token in enumerate(tokens) if 'input_type' in token]\n",
    "            out_indices = [i for i, token in enumerate(tokens) if 'output_type' in token]\n",
    "            \n",
    "            # 在第一个 input_type 之前插入 [INs]\n",
    "            if in_indices:\n",
    "                tokens = tokens[:in_indices[0]] + ['[INs]'] + tokens[in_indices[0]:]\n",
    "            \n",
    "            # 在第一个 output_type 之前插入 [OUTs]\n",
    "            if out_indices:\n",
    "                tokens = tokens[:out_indices[0] + 1] + ['[OUTs]'] + tokens[out_indices[0] + 1:]\n",
    "        \n",
    "        return tokens\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def tokenize_tree(root):\n",
    "    # 递归地对树的每个节点进行标记化\n",
    "    root.data = tokenize_text(root.data,root.tag)\n",
    "    print(root.data)\n",
    "    \n",
    "    for child, edge_data in root.children:\n",
    "        child.data = tokenize_text(child.data,child.tag)\n",
    "        tokenize_tree(child)\n",
    "\n",
    "# 构建词汇表\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.oov_index = 0  # 假设 0 是 [OOV] 的索引\n",
    "        self.word_to_index['[OOV]'] = self.oov_index\n",
    "        self.index_to_word[self.oov_index] = '[OOV]'\n",
    "        self.index = 1\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_index:\n",
    "            self.word_to_index[word] = self.index\n",
    "            self.index_to_word[self.index] = word\n",
    "            self.index += 1\n",
    "\n",
    "    def get_index(self, word):\n",
    "        # 获取单词的索引\n",
    "        if word in self.word_to_index:\n",
    "            return self.word_to_index[word]\n",
    "        else:\n",
    "            # 处理词汇表之外的单词\n",
    "            return self.word_to_index['[OOV]']\n",
    "            \n",
    "def build_vocabulary(root, vocabulary):\n",
    "    for token in root.data:\n",
    "            vocabulary.add_word(token)\n",
    "    # 递归地添加每个节点的单词到词汇表\n",
    "    for child, edge_data in root.children:\n",
    "        for token in child.data:\n",
    "            vocabulary.add_word(token)\n",
    "        build_vocabulary(child, vocabulary)\n",
    "#转化为词向量\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.embed = self.embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * (self.d_model ** 0.5)\n",
    "#生成动态的position embedding\n",
    "def generate_position_embedding(seq_len, d_model):\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe.unsqueeze(0)\n",
    "#继续添加上计算节点的相对位置信息\n",
    "#计算节点的相对深度\n",
    "def depth(node):\n",
    "    if not node:\n",
    "        return 0\n",
    "    d = 0\n",
    "    for child, edge_data in node.children:\n",
    "        d = max(d, depth(child) + 1)\n",
    "    return d\n",
    "def build_tree_position_embedding(root, d_model):\n",
    "    max_depth = depth(root)\n",
    "    position_embedding = torch.zeros(max_depth+1, d_model)\n",
    "    for i in range(max_depth):\n",
    "        for j in range(d_model):\n",
    "            position_embedding[i][j] = math.sin(i / (10000 ** (2 * j / d_model)))\n",
    "    return position_embedding\n",
    "\n",
    "#递归调用转化结点-生成初始的work embedding\n",
    "def build_WordEmbedding(root, vocabulary, vocab_size, d_model):\n",
    "    embedding_layer = WordEmbedding(vocab_size, d_model)    # 将单词embedding化，输出一个模型，等下需要使用这个模型\n",
    "    tensor_from_add = torch.rand(1, d_model).unsqueeze(0)  # 对不同from要加上的形状为 (1, 512) 的张量\n",
    "    tensor_to_add = -tensor_from_add                       # 对不同to要加上的张量#在此处对其加上tree position信息\n",
    "\n",
    "\n",
    "    tree_position_embedding = build_tree_position_embedding(root,d_model)   #生成对应的tree_position\n",
    "\n",
    "\n",
    "    tokens = root.data\n",
    "    token_indices = torch.tensor([[vocabulary.get_index(token) for token in tokens]], dtype=torch.long)\n",
    "    embeddings = embedding_layer(token_indices)\n",
    "    root.embedding = embeddings     ##此处完成基础的token embedding\n",
    "\n",
    "\n",
    "    #获取动态生成的position embedding\n",
    "    max_len = len(tokens)\n",
    "    position_embedding = generate_position_embedding(max_len, d_model)\n",
    "\n",
    "\n",
    "    #将词向量和 position embedding 相加\n",
    "    embeddings = embeddings + position_embedding\n",
    "    root.embedding = embeddings     ##此处完成加上root信息后的的token embeddin\n",
    "\n",
    "    ##在此处对其加上src信息\n",
    "    if(root.tag == 'call'):\n",
    "        embeddings[0, 2, :] +=  tensor_from_add.squeeze().expand_as(embeddings[0, 2, :])\n",
    "        embeddings[0, 3, :] +=  tensor_to_add.squeeze().expand_as(embeddings[0, 3, :])\n",
    "\n",
    "    root.embedding = embeddings     #此处完成了对call trace中的form和to加上信息的操作\n",
    "    root.embedding = root.embedding.reshape(-1, root.embedding.size(-1))\n",
    "    \n",
    "    def trave(root, vocabulary, vocab_size, d_model):\n",
    "        for child, edge_data in root.children:\n",
    "            tokens = child.data\n",
    "            token_indices = torch.tensor([[vocabulary.get_index(token) for token in tokens]], dtype=torch.long)\n",
    "            embeddings = embedding_layer(token_indices)\n",
    "            child.embedding = embeddings    #此处完成基础的token embedding\n",
    "            #获取动态生成的position embedding\n",
    "            max_len = len(tokens)\n",
    "            position_embedding = generate_position_embedding(max_len, d_model)\n",
    "            #将词向量和 position embedding 相加\n",
    "            embeddings = embeddings + position_embedding\n",
    "            child.embedding = embeddings     ##此处完成加上root信息后的的token embeddin\n",
    "            ##在此处对其加上src信息\n",
    "            if(child.tag == 'call'):\n",
    "                embeddings[0, 2, :] += tensor_from_add.squeeze().expand_as(embeddings[0, 2, :])\n",
    "                embeddings[0, 3, :] += tensor_to_add.squeeze().expand_as(embeddings[0, 3, :])\n",
    "            child.embedding = embeddings\n",
    "            #加上tree position\n",
    "            child.embedding += tree_position_embedding[child.deep,:].unsqueeze(0)\n",
    "            child.embedding = child.embedding.reshape(-1, child.embedding.size(-1))\n",
    "            trave(child, vocabulary, vocab_size, d_model)\n",
    "    trave(root, vocabulary, vocab_size, d_model)\n",
    "\n",
    "# 自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.max_seq_length = max(len(node.embedding) for node in data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def pad_sequence(self, sequence, max_length):\n",
    "        # 如果序列长度小于最大长度，进行填充\n",
    "        if len(sequence) < max_length:\n",
    "            padding_size = max_length - len(sequence)\n",
    "            padding = torch.zeros(padding_size, sequence.size(-1))  # 假设最后一维是embedding的维度\n",
    "            sequence = torch.cat([sequence, padding], dim=0)\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取节点embedding\n",
    "        node_embedding = self.data[idx].embedding\n",
    "        # 使用 pad_sequence 方法填充序列到最大长度\n",
    "        padded_embedding = self.pad_sequence(node_embedding, self.max_seq_length)\n",
    "        # 返回节点嵌入信息作为输入和输出型\n",
    "        return padded_embedding, padded_embedding\n",
    "\n",
    "# Transformer 编码器模型\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, embedding_layer,vocab_size,d_model, hidden_size, num_heads, num_layers):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, num_heads, hidden_size), num_layers\n",
    "        )\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# 创建数据集-递归方式\n",
    "def traverse_tree(node, node_list):\n",
    "    # 将当前节点加入列表\n",
    "    node_list.append(node)\n",
    "    # 递归遍历子节点\n",
    "    for child, _ in node.children:\n",
    "        traverse_tree(child, node_list)\n",
    "\n",
    "\n",
    "#数据处理部分函数\n",
    "def process_call_entry(call_entry, call_idx, Seqscall_1, Seqsstate_1, Seqslog_1, state_idx, log_idx):\n",
    "    Seqscall_1[0].append(f't{call_idx}call')\n",
    "    call_info = [f'{call_entry[\"call_from\"]},{call_entry[\"call_to\"]},{call_entry[\"call_function_name\"]},{call_entry[\"call_gas\"]},{call_entry[\"call_value\"]}']\n",
    "\n",
    "    for input_entry in call_entry[\"call_input\"]:\n",
    "        call_info.extend([f'{input_entry[\"call_input_type\"]},{input_entry[\"call_input_value\"]}'])\n",
    "\n",
    "    for output_entry in call_entry[\"call_output\"]:\n",
    "        call_info.extend([f'{output_entry[\"call_output_type\"]},{output_entry[\"call_output_value\"]}'])\n",
    "\n",
    "    Seqscall_1[1].extend([','.join(call_info)])\n",
    "\n",
    "    for state_entry in call_entry[\"stata\"]:\n",
    "        Seqsstate_1[0].append(f't{state_idx}state')\n",
    "        Seqsstate_1[1][f't{state_idx}state'] = f't{call_idx}call'\n",
    "        state_info = [f'{state_entry[\"tag\"]},{state_entry[\"key\"]},{state_entry[\"value\"]}']\n",
    "        Seqsstate_1[2].extend(state_info)\n",
    "        state_idx += 1\n",
    "\n",
    "    for log_entry in call_entry[\"log\"]:\n",
    "        Seqslog_1[0].append(f't{log_idx}log')\n",
    "        Seqslog_1[1][f't{log_idx}log'] = f't{call_idx}call'\n",
    "        log_info = [f'{log_entry[\"contract_address\"]},{log_entry[\"event_hash\"]}']\n",
    "\n",
    "        for l_d_entry in log_entry[\"data\"]:\n",
    "            log_info.extend([f'{l_d_entry[\"type\"]},{l_d_entry[\"value\"]}'])\n",
    "\n",
    "        Seqslog_1[2].extend(log_info)\n",
    "        log_idx += 1\n",
    "    return state_idx,log_idx\n",
    "\n",
    "def build_tree_and_vocabulary(Seqsstate_1, Seqslog_1, Seqscall_1,vocabulary):\n",
    "    # 讲一个交易划分为3个树，3个树是包含在一个根节点中的，根结点就是第一个call，下面一层是第二个call、第一个call中的所有state、第一个call中的所有log\n",
    "    root_node = build_ITR_tree(Seqsstate_1, Seqslog_1, Seqscall_1)\n",
    "    # 将树token化，每个值都保存为一个token\n",
    "    tokenize_tree(root_node)\n",
    "    \n",
    "    build_vocabulary(root_node, vocabulary)\n",
    "    \n",
    "    return root_node, vocabulary\n",
    "\n",
    "def process_entry(entry):\n",
    "    Seqscall_1 = [[],[]]\n",
    "    Seqsstate_1 = [[],{},[]]\n",
    "    Seqslog_1 = [[],{},[]]\n",
    "\n",
    "    state_idx = 0\n",
    "    log_idx = 0\n",
    "    \n",
    "    # 一条entry中一个hash对应多个call\n",
    "    for call_idx, call_entry in enumerate(entry[\"call\"]):\n",
    "        state_idx,log_idx = process_call_entry(call_entry, call_idx, Seqscall_1, Seqsstate_1, Seqslog_1, state_idx, log_idx)\n",
    "\n",
    "    return Seqsstate_1, Seqslog_1, Seqscall_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型推理阶段\n",
    "loaded_model = torch.load('./model/5w_data_23.pth')\n",
    "# 定义损失函数和优化器\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "def is_anomaly(seqscall, seqsstate , seqslog, model, threshold=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():   \n",
    "        eval_tree_node = build_ITR_tree(seqsstate, seqslog, seqscall)\n",
    "        # 标记化树的内容\n",
    "        tokenize_tree(eval_tree_node)\n",
    "        # 构建嵌入层\n",
    "        build_WordEmbedding(eval_tree_node, vocabulary, vocab_size, d_model)\n",
    "        #print(eval_tree_node.embedding)\n",
    "        #print(eval_tree_node.embedding.shape)\n",
    "        #检测\n",
    "        def travel(root_node):\n",
    "            for child,e in root_node.children:\n",
    "                #print(root_node.embedding.shape)\n",
    "                travel(child)\n",
    "        travel(eval_tree_node)\n",
    "\n",
    "        \n",
    "        # 创建一个空列表用于存储所有节点\n",
    "        tree_node_list = []\n",
    "        traverse_tree(eval_tree_node, tree_node_list)\n",
    "        data_loader = CustomDataset(tree_node_list)\n",
    "        #  创建数据加载器\n",
    "        batch_size = 2\n",
    "        myval_loader = DataLoader(data_loader, batch_size=batch_size, shuffle=False)   \n",
    "\n",
    "        for inputs, targets in myval_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            targets = torch.argmax(targets, dim=-1)\n",
    "            loss = loss_function(outputs.permute(0, 2, 1), targets)\n",
    "\n",
    "        #print(loss)\n",
    "        #print(loss.item())\n",
    "        return loss.item(),loss.item() > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试模型推理部分代码\n",
    "\n",
    "# 加载训练模型\n",
    "loaded_model = torch.load('./model/5w_data_23.pth')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#记录训练结果\n",
    "csv_file_path = 'tmp/example.csv'\n",
    "\"\"\"\n",
    "# 提供你的新输入数据\n",
    "with open('./trace_processed.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\"\"\"\n",
    "\n",
    "#连接数据库构建\n",
    "client = MongoClient('mongodb://b515:sqwUiJGHYQTikv6z@10.12.46.33:27018/?authMechanism=DEFAULT')\n",
    "dbtest = client[\"geth\"]\n",
    "collection = dbtest.get_collection(\"5m_10m_5w_output\")\n",
    "data = collection.find()\n",
    "#data = collection.aggregate([{'$sample':{'size':2000}}])\n",
    "\n",
    "print(\"开始推理阶段\")\n",
    "# 从文件中加载实例\n",
    "with open(\"./vocabulary/vocabulary_5w.pkl\",\"rb\") as file:   \n",
    "    vocabulary = pickle.load(file)\n",
    "vocab_size = len(vocabulary)+1\n",
    "# 然后继续进行写入CSV文件的操作\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    # 创建CSV写入器\n",
    "    writer = csv.writer(file)\n",
    "    # 写入表头\n",
    "    writer.writerow(['tx_hash', 'loss', 'answer'])\n",
    "    for idx,entry in enumerate(data):\n",
    "        # 每条交易开始重制Seqscall等信息\n",
    "        # 初始化变量为列表\n",
    "        Seqscall_1= [[],[]]\n",
    "        Seqsstate_1 = [[],{},[]]\n",
    "        Seqslog_1 = [[],{},[]]\n",
    "\n",
    "        state_idx = 0\n",
    "        log_idx = 0\n",
    "        \n",
    "        # 处理调用信息\n",
    "        for call_idx,call_entry in enumerate(entry[\"call\"]):\n",
    "            Seqscall_1[0].append(f't{call_idx}call')\n",
    "            call_info = [f'{call_entry[\"call_from\"]},{call_entry[\"call_to\"]},{call_entry[\"call_function_name\"]},{call_entry[\"call_gas\"]},{call_entry[\"call_value\"]}']\n",
    "            for input_idx,input_entry in enumerate(call_entry[\"call_input\"]):\n",
    "                call_info = call_info + [f'{input_entry[\"call_input_type\"]},{input_entry[\"call_input_value\"]}']\n",
    "                call_info = [','.join(call_info)]\n",
    "            for output_idx,output_entry in enumerate(call_entry[\"call_output\"]):\n",
    "                call_info = call_info + [f'{output_entry[\"call_output_type\"]},{output_entry[\"call_output_value\"]}']\n",
    "                call_info = [','.join(call_info)]\n",
    "            Seqscall_1[1] += call_info\n",
    "            #以上结束对Seqcall处理\n",
    "            for s_idx,state_entry in enumerate(call_entry[\"stata\"]):\n",
    "                Seqsstate_1[0].append(f't{state_idx}state')\n",
    "                Seqsstate_1[1][f't{state_idx}state'] = f't{call_idx}call'\n",
    "                state_info = [f'{state_entry[\"tag\"]},{state_entry[\"key\"]},{state_entry[\"value\"]}']\n",
    "                Seqsstate_1[2] += state_info\n",
    "                state_idx+=1\n",
    "            #以上结束对Seqsstate处理\n",
    "            for l_idx,log_entry in enumerate(call_entry[\"log\"]):\n",
    "                Seqslog_1[0].append(f't{log_idx}log')\n",
    "                Seqslog_1[1][f't{log_idx}log'] = f't{call_idx}call'\n",
    "                log_info = [f'{log_entry[\"contract_address\"]},{log_entry[\"event_hash\"]}']\n",
    "                for l_d_idx,l_d_entry in enumerate(log_entry[\"data\"]):\n",
    "                    log_info = log_info + [f'{l_d_entry[\"type\"]},{l_d_entry[\"value\"]}']\n",
    "                    log_info = [','.join(log_info)]\n",
    "                Seqslog_1[2] += log_info\n",
    "                log_idx+=1\n",
    "            #以上结束对Seqlog处理\n",
    "\n",
    "        # 假设 is_anomaly 函数返回 (loss, answer)\n",
    "        result = is_anomaly(seqsstate=Seqsstate_1, seqslog=Seqslog_1, seqscall=Seqscall_1, model=loaded_model)\n",
    "        # 检查 is_anomaly 返回的结果是否为布尔值\n",
    "        if isinstance(result, bool):\n",
    "        # 处理布尔值的情况，例如设置默认的 loss 和 answer\n",
    "            loss = 0.0\n",
    "            answer = \"Unknown\"\n",
    "        else:\n",
    "            # 解包结果\n",
    "            loss, answer = result\n",
    "            # 逐行写入数据\n",
    "        writer.writerow([entry[\"tx_hash\"], loss, answer])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "        loss,answer=is_anomaly(seqsstate=Seqsstate_1,seqslog=Seqslog_1,seqscall=Seqscall_1,model=loaded_model)\n",
    "        eval_data=[entry[\"tx_hash\"],loss,answer]\n",
    "\n",
    "        print(eval_data)\n",
    "        #写入数据\n",
    "        writer.writerow(eval_data)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pymongo import MongoClient\n",
    "import torch\n",
    "from build_WordEmbedding import build_WordEmbedding\n",
    "from build_tree_and_vocabulary import build_tree_and_vocabulary\n",
    "from build_vocabulary import Vocabulary\n",
    "from data_process import process_entry\n",
    "import time\n",
    "import pickle\n",
    "from traverse_tree import traverse_tree \n",
    "import csv\n",
    "from CutomDataset import CustomDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 连接数据库，读取数据\n",
    "client = MongoClient('mongodb://b515:sqwUiJGHYQTikv6z@10.12.46.33:27018/?authMechanism=DEFAULT')\n",
    "dbtest = client[\"geth\"]\n",
    "collection = dbtest.get_collection(\"5m_10m_50w_output\")\n",
    "data = collection.find()\n",
    "\n",
    "tree_node_list = [] # 把node放入list中\n",
    "tensor_list = {}    # tensor词典，格式是{'tensor_x':embedding}\n",
    "vocabulary = Vocabulary()\n",
    "file_i = 0  # file_i是将数据embedding后保存至第几个文件\n",
    "data_i = 0  # data_i是记录读取的交易个数\n",
    "\n",
    "# 加载训练模型\n",
    "loaded_model = torch.load('./model/5w_data_23.pth')\n",
    "#记录训练结果\n",
    "csv_file_path = 'tmp/example.csv'\n",
    "# 从文件中加载词汇表\n",
    "with open(\"./vocabulary/vocabulary_5w.pkl\",\"rb\") as file:   \n",
    "    vocabulary_1 = pickle.load(file)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "# 创建CSV写入器\n",
    "    writer = csv.writer(file)\n",
    "    for idx, entry in enumerate(data):\n",
    "        Seqsstate_1, Seqslog_1, Seqscall_1 = process_entry(entry)\n",
    "        root_node, vocabulary = build_tree_and_vocabulary(Seqsstate_1, Seqslog_1, Seqscall_1,vocabulary)\n",
    "        vocab_size = vocabulary_1.index + 1\n",
    "        d_model = 64\n",
    "        build_WordEmbedding(root_node, vocabulary, vocab_size, d_model)\n",
    "        traverse_tree(root_node, tree_node_list)  # 将node节点加入到列表中\n",
    "        embedding_file = []\n",
    "        for i in range(len(tree_node_list)):\n",
    "            embedding_file.append(tree_node_list[i].embedding)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():     \n",
    "            custom_data = CustomDataset(embedding_file)\n",
    "            #创建数据加载器\n",
    "            batch_size = 8\n",
    "            data_loader = DataLoader(custom_data, batch_size=batch_size, shuffle=True)\n",
    "            #训练部分\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            # 定义损失函数和优化器\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "            #  创建数据加载器\n",
    "            batch_size = 2\n",
    "            myval_loader = DataLoader(data_loader, batch_size=batch_size, shuffle=False)   \n",
    "\n",
    "            for inputs, targets in myval_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                targets = torch.argmax(targets, dim=-1)\n",
    "                loss = loss_function(outputs.permute(0, 2, 1), targets)\n",
    "\n",
    "        tree_node_list = []\n",
    "        tensor_list = {}\n",
    "        data_i +=1\n",
    "        # 检查 is_anomaly 返回的结果是否为布尔值\n",
    "        if isinstance(result, bool):\n",
    "        # 处理布尔值的情况，例如设置默认的 loss 和 answer\n",
    "            loss = 0.0\n",
    "            answer = \"Unknown\"\n",
    "        else:\n",
    "            # 解包结果\n",
    "            loss, answer = result\n",
    "            # 逐行写入数据\n",
    "        writer.writerow([entry[\"tx_hash\"], loss, loss.item() > threshold])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
